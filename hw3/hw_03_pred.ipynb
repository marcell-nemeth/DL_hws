{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "hQLLaNVxovIE",
    "outputId": "c05564b9-e014-437a-f3ce-3bbbe11cf6aa"
   },
   "outputs": [],
   "source": [
    "# library needed for web crawling\n",
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p2VD2Kl855TF",
    "outputId": "e4140c40-2b59-4414-a986-96797cbf774d"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "# version downgrade needed for easier .npy file IO\n",
    "!pip install numpy==1.16.1\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qTYT8imc5MPB"
   },
   "outputs": [],
   "source": [
    "t_max=[]\n",
    "t_min=[]\n",
    "t_avg=[]\n",
    "rain=[]\n",
    "dates=[]\n",
    "\n",
    "# this is where I downloaded the weather data from\n",
    "webpage_base_addr='https://www.metnet.hu/napi-adatok?sub=4&pid=10602&date='\n",
    "webpage_test='https://www.metnet.hu/napi-adatok?sub=4&pid=10602&date=2017-10-25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Cq3XYoV7AM6c",
    "outputId": "fcd3248b-89ae-4377-fd9d-f5e4b76d9af9"
   },
   "outputs": [],
   "source": [
    "# I created an own html parser dedicated for metnet.hu\n",
    "\n",
    "for year in range(2015,2020): # fetching data of last 5 years\n",
    "  if year<2019:\n",
    "    for month in range(1,13):      \n",
    "      act_date=str(year)+'-'+str(month)+'-'+'1' \n",
    "      page_html=urlopen(webpage_base_addr+act_date) # the script rolls over the different addresses of the dates, addresses end to the specific date\n",
    "\n",
    "      print(webpage_base_addr+act_date)\n",
    "\n",
    "      soup = BeautifulSoup(page_html, 'html.parser') # parsed the whole html source\n",
    "      js_part=soup.find_all('script')[7] # data was stored in JS arrays, so I had to find the JS part first, than extract the array from it\n",
    "      js_text=js_part.get_text().replace('\"','') # needed to make array conversion easier\n",
    "      raw_data=re.findall(r\"(?<=data: ).*?(?=,\\n)\", js_text)[:-2] # needed to fid the data JS part for value extracton\n",
    "      print(raw_data)\n",
    "\n",
    "      for i in range(len(raw_data)): # raw_data contains max, min, avg, rain values\n",
    "        raw_data[i]=raw_data[i][1:-1].split(',') # splitting values\n",
    "        raw_data[i]=[float(j) for j in raw_data[i]] # converting to number values\n",
    "\n",
    "      t_max.append(raw_data[0])\n",
    "      t_min.append(raw_data[1])\n",
    "      t_avg.append(raw_data[2])\n",
    "      rain.append(raw_data[3])\n",
    "      print(act_date)\n",
    "      \n",
    "      for day in range(1,len(raw_data[0])+1):\n",
    "        dates.append(str(year)+'-'+str(month)+'-'+str(day)) # created a date column, timestamp\n",
    "      \n",
    "  else: # this case handles the current year, we are only in October, so the loop rolling over the entire year in previous years would have given an error telling that metnet.hu doesnt have the values for the rest of the year\n",
    "    for month in range(1,11):        \n",
    "      act_date=str(year)+'-'+str(month)+'-'+'1'\n",
    "      page_html=urlopen(webpage_base_addr+act_date)\n",
    "\n",
    "      print(webpage_base_addr+act_date)\n",
    "\n",
    "      soup = BeautifulSoup(page_html, 'html.parser')\n",
    "      js_part=soup.find_all('script')[7]\n",
    "      js_text=js_part.get_text().replace('\"','')\n",
    "      raw_data=re.findall(r\"(?<=data: ).*?(?=,\\n)\", js_text)[:-2]\n",
    "      print(raw_data)\n",
    "\n",
    "      for i in range(len(raw_data)):\n",
    "        raw_data[i]=raw_data[i][1:-1].split(',')\n",
    "        raw_data[i]=[float(j) for j in raw_data[i]]\n",
    "\n",
    "      t_max.append(raw_data[0])\n",
    "      t_min.append(raw_data[1])\n",
    "      t_avg.append(raw_data[2])\n",
    "      rain.append(raw_data[3])\n",
    "      print(act_date) \n",
    "      \n",
    "      for day in range(1,len(raw_data[0])+1):\n",
    "        dates.append(str(year)+'-'+str(month)+'-'+str(day))\n",
    "        \n",
    "# saving arrays to .npy files        \n",
    "np.save('t_max',t_max)\n",
    "np.save('t_min',t_min)\n",
    "np.save('t_avg',t_avg)\n",
    "np.save('rain',rain)\n",
    "np.save('dates', dates)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WktxJjNGHM_8"
   },
   "outputs": [],
   "source": [
    "# loading back from .npy files\n",
    "t_max=[item for sublist in np.load('t_max.npy') for item in sublist]\n",
    "t_min=[item for sublist in np.load('t_min.npy') for item in sublist]\n",
    "t_avg=[item for sublist in np.load('t_avg.npy') for item in sublist]\n",
    "rain=[item for sublist in np.load('rain.npy') for item in sublist]\n",
    "dates=np.load('dates.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPvfHRzCJDLv"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(list(zip(dates,t_max,t_min,t_avg,rain)), columns=['Date','T_max','T_min','T_avg','Rain']) # creating Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "31qNyVk2UDMP"
   },
   "outputs": [],
   "source": [
    "df.to_csv('weather_data.csv') # saving to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PjmvSCeUTMc"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('weather_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "9jzllrP6viLE",
    "outputId": "4f2c88af-c8aa-43fd-ab30-341baa267f76"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "colab_type": "code",
    "id": "mC2-9AhQx1nK",
    "outputId": "b9fd41f6-8b19-49cb-fa7f-eb0f7ec957e5"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOCwSIeo19rm"
   },
   "outputs": [],
   "source": [
    "window_size = 5 # look-back window, I chose a size of 5 former values to predict a 6th one\n",
    "generator = TimeseriesGenerator(df['T_avg'].values, df['T_avg'].values, length=window_size, batch_size=1) # the generator made the train set creation easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ihdMeOZK2OhM",
    "outputId": "434b4859-1d5b-471a-f2a5-95360390bad8"
   },
   "outputs": [],
   "source": [
    "# the generator assigns 5 former values to a 6th one\n",
    "for i in range(len(generator)):\n",
    "\tx, y = generator[i]\n",
    "\tprint('%s => %s' % (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KqLfNmq02Uoj"
   },
   "outputs": [],
   "source": [
    "# creating model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_dim=window_size))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdpxjVFu3Wd7"
   },
   "outputs": [],
   "source": [
    "# training model\n",
    "model.fit_generator(generator, steps_per_epoch=1, epochs=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NpnstlFT1Mnp"
   },
   "outputs": [],
   "source": [
    "#test input\n",
    "x_input = df['T_avg'][-1*window_size-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mVrlqOHr3xw5",
    "outputId": "5b37214a-3bcf-4c00-8db9-1476857869ed"
   },
   "outputs": [],
   "source": [
    "x_input = x_input.values.reshape((1, window_size))\n",
    "yhat = model.predict(x_input, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Zcpg03-u1edc",
    "outputId": "85e20fcd-5a59-4aa3-dbfa-0f654d52c8c0"
   },
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjllyrCO4tyr"
   },
   "outputs": [],
   "source": [
    "df_extend=copy.copy(df['T_avg'].values)\n",
    "df_extend2=copy.copy(df['T_avg'].values)\n",
    "df_extend3=copy.copy(df['T_avg'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rb_KXF2w-Fck"
   },
   "outputs": [],
   "source": [
    "# dates wished to predict\n",
    "date1 = date(2019, 10, 30)\n",
    "date2 = date(2019, 11, 5)\n",
    "date3 = date(2019, 11, 26)\n",
    "\n",
    "# remaining days until the specific dates\n",
    "days_left1=(date1-date.today()).days\n",
    "days_left2=(date2-date.today()).days\n",
    "days_left3=(date3-date.today()).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwRxAwh2CMQu"
   },
   "outputs": [],
   "source": [
    "# my main idea was to predict the average temperature from the past 5 days' average temperature\n",
    "# that's why I needed to predict all the upcoming temperature values until the specific day, so I had to make predictions \"days_left{n}\" times\n",
    "for day_pred in range(days_left1):\n",
    "  generator = TimeseriesGenerator(df_extend, df_extend, length=window_size, batch_size=1) # generating updated training set\n",
    "  model.fit_generator(generator, steps_per_epoch=1, epochs=200, verbose=1)\n",
    "  df_input=copy.copy(df_extend[-1*window_size:])\n",
    "  df_input=df_input.reshape((1, window_size))\n",
    "  yhat = model.predict(df_input, verbose=1) # here comes the prediction\n",
    "  print(yhat)\n",
    "  df_extend=np.append(df_extend,yhat) # adding the predicted value to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "v4yONyxn29lI",
    "outputId": "8561fc7f-538e-4a9f-80d3-88682d8d73b2"
   },
   "outputs": [],
   "source": [
    "print('Predicted temperature for 30th of October: %s Celsius' % (np.round(df_extend[-1],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g7bvszUC3Lmq"
   },
   "outputs": [],
   "source": [
    "# date2 prediction\n",
    "for day_pred in range(days_left2):\n",
    "  generator = TimeseriesGenerator(df_extend2, df_extend2, length=window_size, batch_size=1)\n",
    "  model.fit_generator(generator, steps_per_epoch=1, epochs=200, verbose=1)\n",
    "  df_input=copy.copy(df_extend2[-1*window_size:])\n",
    "  df_input=df_input.reshape((1, window_size))\n",
    "  yhat = model.predict(df_input, verbose=1)\n",
    "  print(yhat)\n",
    "  df_extend2=np.append(df_extend2,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZZSBcvYTBCr3"
   },
   "outputs": [],
   "source": [
    "np.save('model2',df_extend2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "79TcDL5C6LV1",
    "outputId": "09d0a684-c65a-40ab-c519-fc598f6d5051"
   },
   "outputs": [],
   "source": [
    "print('Predicted temperature for 5th of November: %s Celsius' % (np.round(df_extend2[-1],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "DL5d1CQy_Brk",
    "outputId": "a29a7315-2e99-49b5-8f61-20c5b742f135"
   },
   "outputs": [],
   "source": [
    "# predicted values highighted in red\n",
    "plt.plot(df_extend2[-20:])\n",
    "plt.axvspan(20-days_left2,20, color='red', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCXhuZrA6zAj"
   },
   "outputs": [],
   "source": [
    "# date3 prediction\n",
    "for day_pred in range(days_left3):\n",
    "  generator = TimeseriesGenerator(df_extend3, df_extend3, length=window_size, batch_size=1)\n",
    "  model.fit_generator(generator, steps_per_epoch=1, epochs=200, verbose=1)\n",
    "  df_input=copy.copy(df_extend3[-1*window_size:])\n",
    "  df_input=df_input.reshape((1, window_size))\n",
    "  yhat = model.predict(df_input, verbose=1)\n",
    "  print(yhat)\n",
    "  df_extend3=np.append(df_extend3,yhat)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qZ_UbDjtBAOZ"
   },
   "outputs": [],
   "source": [
    "np.save('model3',df_extend3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jA0SGsMA7O8W",
    "outputId": "3580f99c-36ad-4ef4-df4a-7f7ecf9b9f4a"
   },
   "outputs": [],
   "source": [
    "print('Predicted temperature for 26th of November: %s Celsius' % (np.round(df_extend3[-1],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "gbVuOJTd7Q_W",
    "outputId": "4dfa08bd-95bf-416e-c749-f76aeb29ef09"
   },
   "outputs": [],
   "source": [
    "plt.plot(df_extend3[-50:])\n",
    "plt.axvspan(50-days_left3,50, color='red', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "83Ex9eFLBLrj"
   },
   "source": [
    "# To be honest I found this task inappropriate for a neural network. The task could have been solved with other ML tools like decision trees a lot easier and more precisely. Basically the NN had to learn the trend of the weather time series which is possible, but comes with the difficulty of choosing the window size correctly. As far as I see the most important hyper-parameter here is the training window size as the network could easily become over fitted. In our case I chose a relatively small window of 5 days, on the plots it's clearly visible that the NN learnt the trend based on te last week which was quite warm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw_03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
